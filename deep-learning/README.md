# Deep Learning

* [Feed Forward Networks](feed-forward.pdf)
* [Backpropagation](backprop.pdf)
* [Optimization](optimization.pdf)
* [TODO RNNs]: [General LSTM overview](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), [LSTM backprop derivation](http://arunmallya.github.io/writeups/nn/lstm/index.html#/), LSTM dropout (Bayer and Osendorfer, 2014; Pascanu et al., 2014a), [LSTM intro blog post](http://blog.echen.me/2017/05/30/exploring-lstms/?imm_mid=0f2ce7&cmp=em-data-na-na-newsltr_20170614), [Karpathy LSTM](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Also RBMs, though they are weaker than RNNs (Hinton Coursera lecture 2) [notes1](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/).
* [TODO VAEs]: [notes1](https://arxiv.org/abs/1606.05908), [notes2](http://kvfrans.com/variational-autoencoders-explained/), [notes3](http://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models)

