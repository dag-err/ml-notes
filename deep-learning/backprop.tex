\documentclass{article}

\title{Backpropagation}
\author{Vladimir Feinberg}

\input{../defs}

\begin{document}

\maketitle

Neural networks are real-valued circuits constructed from simple functions. This compositionality enables complex expressiveness of the function family through hidden units, while permitting efficient function evaluation and gradient evaluation. This trick is so essential to NN training that it is worthwhile to apply the trick to specific NN instances to understand how the flow of gradients travels.

[TODO follow 6.5 in DL book], give derivations for all DNNs, convolutions, etc in a new doc. Resources probably exist online.

[TODO Batch Normalization backprop], see \nurl{http://cthorey.github.io/backpropagation/}{notes here}, but they include bias...

\end{document}
