\documentclass{article}

\title{Complexity Measures}
\author{Vladimir Feinberg}

\input{defs}

\begin{document}

\maketitle

Complexity measures evaluate the expressiveness of a hypothesis class; they are useful to the extent with which they relate sample and generalization error.

\section{Setup}

We suppose that our data comes in the form of ordered pairs from $\mcX\times \mcY$. Samples follow a particular distribution $(x, y)\sim D$. A hypothesis class $\mcH$ is set of functions $\mcX\rightarrow\mcY$.

A common approach to supervised learning is ERM, where $m$ iid samples from $D$, $S$, are used to find the $h\in\mcH$ minimizing a specified loss $\ell:\mcY^2\rightarrow\R$ over this set. Complexity measures then let us quanify exactly how much loss we can expect when sampling from $D$ again.

We seek to quantify the generalization gap with the help of our notions of complexity. For a fixed $h\in \mcH$:

$$
\varepsilon= \E\left[\ell\pa{h(x), y)}|(x,y)\sim D\right]-\E\left[\ell\pa{h(x), y)}|(x,y)\sim \Uniform(S)\right]
$$

Analysis of Rademacher complexity is agnostic to $h,\ell$; the hypothesis class might as well consist of functions $g:\mcX\times\mcY\rightarrow\R$ yielding their composition. VC dimension analysis, however, requires $\mcY=\{0, 1\}$ and $\ell(a, b)=\indicator\{a=b\}$. VC dimension is still useful for regression problems, by thresholding hypotheses $h\mapsto \indicator{h>\beta}$ for fixed $\beta$.\footnote{\url{https://stats.stackexchange.com/questions/140430}}


Thus, it is useful to find bounds on $\varepsilon$, the difference between the generalization loss $\E\left[\ell\pa{h(x), y)}\right]$, where $(x,y)\sim D$, and sample loss, where the loss is the expectation before taken for $(x,y)$ is uniform over $S$.

Let the gap between the generalization and sample error be $\varepsilon$.

\section{Complexity Measures}

Rademacher complexity. TODO. relates complexity through noise correlation. equivalently, gives a consistent view of ``error'' through a ``uniform'' noise model.

VC-dimension. TODO.

\section{Overview of Results}

Proofs can be found in a cogent write-up by Prof. Beckage from the University of Kansas, copied into this repository as \texttt{complexity-measures-beckage.pdf}.\footnote{\url{http://ittc.ku.edu/~beckage/ml800/VC_dim.pdf}}

\subsection{VC Generalization Bounds}

Upper bound. If $d$ is the VC-dimension of $\mcH$, then for any $D$ wp $1-\delta$:
$$
\varepsilon\le \tilde{O}\pa{\sqrt{\frac{d-\log \delta}{m}}}
$$
The above inequality is random since it depends on $S$, the $D^m$-valued rv. TODO. find source removing tilde?

Agnostic lower bound. We may find a $D$ such that with a fixed nonzero probability (a nonneglible set of candidate samples $S$), the following holds:
$$
\varepsilon\ge \Omega\pa{\sqrt{\frac{d}{m}}}
$$

The above implies that in the common case of agnostic hypothesis learning, where we do not know distribution $D$, VC-dimension is, \textit{up to logarithmic factors, asymptotically efficient} in quantifying the generalization gap.

Realizability. Suppose $D$ is realizable wrt $\mcH$, so that almost surely there exists an $f\in\mcH$ such that for any $(x,y)$ sampled from $D$, $f(x)=y$. Then all statements above hold but with $\sqrt{\varepsilon}$ instead of $\varepsilon$.


\subsection{Rademacher bounds}

With $R_m$ either the empirical or expected Rademacher complexity over the sample for a given $h,\ell$ we have again wp $1-\delta$:
$$
\varepsilon\le 2R_m+O\pa{\frac{\log\nicefrac{1}{\delta}}{m}}
$$
$R_m$ may be NP-hard to compute, depending on $\mcH$. This tells us Rademacher complexity could only be a useful imporvement over VC-bounds, asymptotically, if we have an efficient approximation for the empirical Rademacher complexity or some knowledge of $D$ as required to compote the true Rademacher complexity.

TODO. NP-hardness? More computational learning theory.

TODO. Rademacher and Gaussian Complexities: Risk Bounds and Structural Results by Bartlett and Mendelson.

TODO. Growth function as rademacher/VC bridge. Graph of (log Growth func) vs m linear then logarithmic past d. Note no nuance in number of shatterable sets, just that one exists [missing direction for analysis?]
\end{document}